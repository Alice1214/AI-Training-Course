## RS16 Thinking
**1. 机器学习中的监督学习、非监督学习、强化学习有何区别**

答：强化学习是智能体从环境到行为映射的学习，目标是让奖励最大化。监督学习拥有已经准备好的训练数据输出值，强化学习只有奖励值，但是这个奖励值和监督学习的输出值不一样，它不是事先给出的，而是延后给出的；非监督学习既没有输出值也没有奖励值，只有数据特征，而强化学习有奖励值（为负是为惩罚）；非监督学习与监督学习数据之间是独立的，而强化学习数据间具有前后依赖关系。

**2. 什么是策略网络，价值网络，有何区别**

答：策略网络是直接学习策略本身，得到策略函数。对于给定的输入，通过学习给出一个确定输出的网络：（动作1，状态1），（动作2，状态2）；价值网络是通过学习价值函数指导策略的制定：计算目前状态的累积分数的期望，价值网络给游戏中的状态赋予一个分数（数值），每个状态都经历了整个数值网络，奖励更多的状态，会在数值网络中的数值更大。

**3.  请简述MCTS（蒙特卡洛树搜索）的原理，4个步骤Select, Expansion，Simluation，Backpropagation是如何操作的**

答：Step1，选择Select，从根节点往下走，每次都选一个“最有价值的子节点”，直到找到“存在未扩展的子节点”，即这个局面存在未走过的后续着法的节点，比如 3/3 节点

Step2，扩展Expansion，给这个节点加上一个 0/0 子节点，对应之前所说的“未扩展的子节点”

Step3，模拟Simluation，用快速走子策略（Rollout policy）走到底，得到一个胜负结果

Step4，回传Backup，把模拟的结果加到它的所有父节点上，假设模拟的结果是 0/1，就把0/1加到所有父节点上 

**4.  假设你是抖音的技术负责人，强化学习在信息流推荐中会有怎样的作用，如果要进行使用强化学习，都有哪些要素需要考虑**

答：

搜索场景：用户浏览行为可以看成Markov过程，对马尔科夫决策过程进行建模，实现基于强化学习的排序决策决策模型，可以使得搜索更加智能化。

推荐场景：使用强化学习与自适应在线学习，通过持续机器学习和模型优化建立决策引擎，对海量用户行为以及item特征进行实时分析，帮用户快速发现喜欢的item。

需要考虑的是如何构建智能体与环境的反馈机制，随机探索所带来的短期损失是无法完全避免的，因此也要考虑最终探索所带来的收益能不能够弥补并超过其带来的损失。

**5.  在自动驾驶中，如何使用强化学习进行训练，请说明简要的思路**

答：首先是构建强化学习模型, 定义自动驾驶过程中的状态(state)、动作(action)、奖励(reward)等：状态主要通过车辆的传感器获取，如当前时刻车速，周围行驶环境，交通状况等，动作即为车辆各种操作，如加速减速刹车左转右转倒车停车等，奖励是根据环境对于动作的反馈制定的，如车身平稳性、有无碰触障碍物、是否遵守了交通规则；然后利用大量的行车数据进行模型训练，为了安全性，可以在仿真引擎（如UE4）中进行模型训练。

