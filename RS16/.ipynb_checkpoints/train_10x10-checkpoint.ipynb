{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于强化学习的五子棋AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Content：采用强化学习（策略价值网络），训练五子棋AI；棋盘大小 10 * 10\n",
    "* Author:  HuiHui\n",
    "* Date:    2020-06-04\n",
    "* Reference:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **强化学习：**  \n",
    "    &emsp;1. 强化学习四要素：状态(state)、动作(action)、策略（policy）、奖励(reward)  \n",
    "    &emsp;2. RL考虑的是个体（Agent）与环境（Environment）的交互问题，目标是找到一个最优策略，使Agent获得尽可能多的来自环境的奖励  \n",
    "    &emsp;3. 强化学习Agent：基于价值的强化学习、基于策略强化学习、结合策略梯度以及价值函数的强化学习\n",
    "    \n",
    "* **实现思路：**  \n",
    "    &emsp;下棋对弈场景是环境，每一种棋盘布局就是一个状态，所有可能的落子位置是动作空间，落子概率为策略，胜率就是奖励；采用策略价值网络训练AI,使用蒙特卡洛树搜索（MCTS）来进行策略优化,MCTS通过self－play来生成数据供深度神经网络学习,神经网络学习输入为当前棋盘，输出为双端口，分别表示当前棋盘的状态值（value）和当前棋盘各个位置的走子的概率\n",
    "\n",
    "* **项目文件结构：**  \n",
    "    &emsp;1. game_output.py：定义了游戏的棋盘、获取棋盘状态、下棋、判断是否有人胜利、绘制人机对弈的可视化棋盘、自我对弈；  \n",
    "    &emsp;2. human_play_output.py：人机对弈，在可视化界面下实现与AI的对弈；  \n",
    "    &emsp;3. mcts_alphaZero.py：实现Alpha_Zero中的MCTS，使用策略网络来指导树搜索并计算叶节点；  \n",
    "    &emsp;4. mcts_pure.py：实现随机走子策略的MCTS；  \n",
    "    &emsp;5. policy_value_net_pytorch.py：策略价值网络，用来指导MCTS搜索并计算叶子节点；  \n",
    "    &emsp;6. train_10x10.py：训练AI主程序;  \n",
    "    &emsp;7. best_policy_10x10：10X10棋盘下训练得到的AI最优策略模型；  \n",
    "    &emsp;8. demo_10x10.ipynb：10x10棋盘下未采用可视化界面的人机对弈过程；  \n",
    "    &emsp;9. 五子棋人机对弈.png：五子棋人机对弈过程截图。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 安装pytorch(本机)等库\n",
    "* 人机对弈截图\n",
    "* 修改原有代码为10*10\n",
    "* 创建demo_10*10文件\n",
    "* 生成五子棋AI模型 .model  \n",
    "* 进行人机对弈  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入相关库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !/usr/bin/env python\n",
    "# -*- coding=utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time, datetime\n",
    "from surprise import SVD,SVDpp #surprise是推荐算法python实现库\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import Reader\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import KFold\n",
    "from surprise.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 矩阵分解的作用是补全矩阵缺失的评分，但是当预测集中出现训练集中没有的新用户或新电影，矩阵分解(SVD)好像也能得到预测评分，这是为什么呢？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
