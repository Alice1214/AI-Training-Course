{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RepeatBuyers_xgboost.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrVLObec1Tr2",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "* Content：天猫用户复购预测,使用传统机器学习(xgboost)完成预测\n",
        "* Author:  HuiHui\n",
        "* Date:    2020-03-28\n",
        "* Reference:\n",
        "* 数据集：该数据集包含“双11”前6个月和“双11”当天匿名用户的购物日志，以及显示他们是否为重复购买者的标签信息。\n",
        "    * label: 1'表示'user_id'是'merchant_id'的重复买家，而'0'则相反。'-1'表示'user_id'不是给定商家的新客户\n",
        "    * activity_log: {用户id，商家id}之间的一组交互记录，其中每个记录都是一个动作，表示为“项目id:category id:brand id:time\\u stamp:action\\u type”#'用于分隔两个相邻元素。记录不按任何特定顺序排序"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwMlZl6YO2DU",
        "colab_type": "code",
        "outputId": "5f151759-fe05-4368-a88b-1a7cb2f9c2aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive') #挂载网盘"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQ7SClDsRPeA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/gdrive/My Drive/RS/Repeat Buyers Prediction\") #改变当前工作目录到指定的路径"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06hB_RrTSExt",
        "colab_type": "code",
        "outputId": "da55214a-bef3-4085-d7c3-8e13b3a18b62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "!pip install xgboost\n",
        "!pip install scikit_learn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.6/dist-packages (0.90)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.18.2)\n",
            "Requirement already satisfied: scikit_learn in /usr/local/lib/python3.6/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit_learn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit_learn) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit_learn) (1.18.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgluqI-xSiKC",
        "colab_type": "code",
        "outputId": "4b316762-3a00-4f19-b847-0f58c5f4a277",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pandas as pd\n",
        "import gc\n",
        "import time\n",
        "\n",
        "time1=time.time()\n",
        "# 用户行为，使用format1进行加载\n",
        "# # 加载全量样本\n",
        "# user_log = pd.read_csv('./Repeat Buyers Prediction DataSet/data_format1/user_log_format1.csv', dtype={'time_stamp':'str'})\n",
        "# user_info = pd.read_csv('./Repeat Buyers Prediction DataSet/data_format1/user_info_format1.csv')\n",
        "# train_data1 = pd.read_csv('./Repeat Buyers Prediction DataSet/data_format1/train_format1.csv')\n",
        "# submission = pd.read_csv('./Repeat Buyers Prediction DataSet/data_format1/test_format1.csv')\n",
        "\n",
        "# 加载小样本\n",
        "user_log = pd.read_csv('./Repeat Buyers Prediction DataSet/data_format1_small/sample_user_log.csv', dtype={'time_stamp':'str'})\n",
        "user_info = pd.read_csv('./Repeat Buyers Prediction DataSet/data_format1_small/sample_user_info.csv')\n",
        "train_data1 = pd.read_csv('./Repeat Buyers Prediction DataSet/data_format1_small/train.csv')\n",
        "submission = pd.read_csv('./Repeat Buyers Prediction DataSet/data_format1_small/test.csv')\n",
        "\n",
        "train_data = pd.read_csv('./Repeat Buyers Prediction DataSet/data_format2/train_format2.csv') # ???\n",
        "\n",
        "###### 数据处理 #######\n",
        "train_data1['origin'] = 'train'\n",
        "submission['origin'] = 'test'\n",
        "matrix = pd.concat([train_data1, submission], ignore_index=True, sort=False)\n",
        "matrix.drop(['prob'], axis=1, inplace=True)\n",
        "# 连接user_info表，通过user_id关联\n",
        "matrix = matrix.merge(user_info, on='user_id', how='left') # left只保留左边的主键，只在右边主键中存在的行就不取了\n",
        "# 使用merchant_id（原列名seller_id）\n",
        "user_log.rename(columns={'seller_id':'merchant_id'}, inplace=True)\n",
        "\n",
        "# 格式化\n",
        "user_log['user_id'] = user_log['user_id'].astype('int32')\n",
        "user_log['merchant_id'] = user_log['merchant_id'].astype('int32')\n",
        "user_log['item_id'] = user_log['item_id'].astype('int32')\n",
        "user_log['cat_id'] = user_log['cat_id'].astype('int32')\n",
        "user_log['brand_id'].fillna(0, inplace=True)\n",
        "user_log['brand_id'] = user_log['brand_id'].astype('int32')\n",
        "user_log['time_stamp'] = pd.to_datetime(user_log['time_stamp'], format='%H%M')\n",
        "# 1 for <18; 2 for [18,24]; 3 for [25,29]; 4 for [30,34]; 5 for [35,39]; 6 for [40,49]; 7 and 8 for >= 50; 0 and NULL for unknown\n",
        "matrix['age_range'].fillna(0, inplace=True)\n",
        "# 0:female, 1:male, 2:unknown\n",
        "matrix['gender'].fillna(2, inplace=True)\n",
        "matrix['age_range'] = matrix['age_range'].astype('int8')\n",
        "matrix['gender'] = matrix['gender'].astype('int8')\n",
        "matrix['label'] = matrix['label'].astype('str')\n",
        "matrix['user_id'] = matrix['user_id'].astype('int32')\n",
        "matrix['merchant_id'] = matrix['merchant_id'].astype('int32')\n",
        "\n",
        "del user_info, train_data1 #s删除变量，解除变量对数据对象的引用；del删除的是变量，而不是数据\n",
        "gc.collect() #清理内存\n",
        "\n",
        "# User特征处理\n",
        "groups = user_log.groupby(['user_id'])\n",
        "# 用户交互行为数量 u1\n",
        "temp = groups.size().reset_index().rename(columns={0:'u1'})\n",
        "matrix = matrix.merge(temp, on='user_id', how='left')\n",
        "# 使用agg 基于列的聚合操作，统计唯一值的个数 item_id, cat_id, merchant_id, brand_id\n",
        "#temp = groups['item_id', 'cat_id', 'merchant_id', 'brand_id'].nunique().reset_index().rename(columns={'item_id':'u2', 'cat_id':'u3', 'merchant_id':'u4', 'brand_id':'u5'})\n",
        "temp = groups['item_id'].agg([('u2', 'nunique')]).reset_index()\n",
        "matrix = matrix.merge(temp, on='user_id', how='left')\n",
        "temp = groups['cat_id'].agg([('u3', 'nunique')]).reset_index()\n",
        "matrix = matrix.merge(temp, on='user_id', how='left')\n",
        "temp = groups['merchant_id'].agg([('u4', 'nunique')]).reset_index()\n",
        "matrix = matrix.merge(temp, on='user_id', how='left')\n",
        "temp = groups['brand_id'].agg([('u5', 'nunique')]).reset_index()\n",
        "matrix = matrix.merge(temp, on='user_id', how='left')\n",
        "# 时间间隔特征 u6 按照小时\n",
        "temp = groups['time_stamp'].agg([('F_time', 'min'), ('L_time', 'max')]).reset_index()\n",
        "temp['u6'] = (temp['L_time'] - temp['F_time']).dt.seconds/3600\n",
        "matrix = matrix.merge(temp[['user_id', 'u6']], on='user_id', how='left')\n",
        "# 统计操作类型为0，1，2，3的个数\n",
        "temp = groups['action_type'].value_counts().unstack().reset_index().rename(columns={0:'u7', 1:'u8', 2:'u9', 3:'u10'}) #使用unstack（）对计数结果进行重塑;类似于转置\n",
        "matrix = matrix.merge(temp, on='user_id', how='left')\n",
        "\n",
        "# 商家特征处理\n",
        "groups = user_log.groupby(['merchant_id'])\n",
        "# 商家被交互行为数量 m1\n",
        "temp = groups.size().reset_index().rename(columns={0:'m1'})\n",
        "matrix = matrix.merge(temp, on='merchant_id', how='left')\n",
        "# 统计商家被交互的user_id, item_id, cat_id, brand_id 唯一值\n",
        "temp = groups['user_id', 'item_id', 'cat_id', 'brand_id'].nunique().reset_index().rename(columns={'user_id':'m2', 'item_id':'m3', 'cat_id':'m4', 'brand_id':'m5'})\n",
        "matrix = matrix.merge(temp, on='merchant_id', how='left')\n",
        "# 统计商家被交互的action_type 唯一值\n",
        "temp = groups['action_type'].value_counts().unstack().reset_index().rename(columns={0:'m6', 1:'m7', 2:'m8', 3:'m9'})\n",
        "matrix = matrix.merge(temp, on='merchant_id', how='left')\n",
        "# 按照merchant_id 统计随机负采样的个数\n",
        "temp = train_data[train_data['label']==-1].groupby(['merchant_id']).size().reset_index().rename(columns={0:'m10'}) #统计全局训练数据中标签为-1的。不太懂xgboost中为什么统计这个值？？？随机？？？\n",
        "matrix = matrix.merge(temp, on='merchant_id', how='left')\n",
        "\n",
        "# 按照user_id, merchant_id分组\n",
        "groups = user_log.groupby(['user_id', 'merchant_id'])\n",
        "temp = groups.size().reset_index().rename(columns={0:'um1'}) #统计行为个数\n",
        "matrix = matrix.merge(temp, on=['user_id', 'merchant_id'], how='left')\n",
        "temp = groups['item_id', 'cat_id', 'brand_id'].nunique().reset_index().rename(columns={'item_id':'um2', 'cat_id':'um3', 'brand_id':'um4'}) #统计item_id, cat_id, brand_id唯一个数\n",
        "matrix = matrix.merge(temp, on=['user_id', 'merchant_id'], how='left')\n",
        "temp = groups['action_type'].value_counts().unstack().reset_index().rename(columns={0:'um5', 1:'um6', 2:'um7', 3:'um8'})#统计不同action_type唯一个数\n",
        "matrix = matrix.merge(temp, on=['user_id', 'merchant_id'], how='left')\n",
        "temp = groups['time_stamp'].agg([('first', 'min'), ('last', 'max')]).reset_index()\n",
        "temp['um9'] = (temp['last'] - temp['first']).dt.seconds/3600\n",
        "temp.drop(['first', 'last'], axis=1, inplace=True)\n",
        "matrix = matrix.merge(temp, on=['user_id', 'merchant_id'], how='left') #统计时间间隔\n",
        "\n",
        "#用户购买点击比\n",
        "matrix['r1'] = matrix['u9']/matrix['u7'] \n",
        "#商家购买点击比\n",
        "matrix['r2'] = matrix['m8']/matrix['m6'] \n",
        "#不同用户不同商家购买点击比\n",
        "matrix['r3'] = matrix['um7']/matrix['um5']\n",
        "matrix.fillna(0, inplace=True)\n",
        "# # 修改age_range字段名称为 age_0, age_1, age_2... age_8\n",
        "temp = pd.get_dummies(matrix['age_range'], prefix='age') #get_dummies-》onehot编码,prefix表示加前缀age-\n",
        "matrix = pd.concat([matrix, temp], axis=1)\n",
        "temp = pd.get_dummies(matrix['gender'], prefix='g')\n",
        "matrix = pd.concat([matrix, temp], axis=1)\n",
        "matrix.drop(['age_range', 'gender'], axis=1, inplace=True)\n",
        "print(matrix)\n",
        "\n",
        "# 分割训练数据和测试数据\n",
        "train_data = matrix[matrix['origin'] == 'train'].drop(['origin'], axis=1)\n",
        "test_data = matrix[matrix['origin'] == 'test'].drop(['label', 'origin'], axis=1)\n",
        "train_X, train_y = train_data.drop(['label'], axis=1), train_data['label']\n",
        "del temp, matrix\n",
        "gc.collect()\n",
        "\n",
        "# 使用机器学习工具\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import classification_report\n",
        "import xgboost as xgb\n",
        "# 将训练集进行切分，20%用于验证\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(train_X, train_y, test_size=0.2)\n",
        "\n",
        "##### 使用XGBoost ######\n",
        "model = xgb.XGBClassifier(\n",
        "    max_depth=8,\n",
        "    n_estimators=1000,\n",
        "    min_child_weight=300, \n",
        "    colsample_bytree=0.8, \n",
        "    subsample=0.8, \n",
        "    eta=0.35, #学习率调小，训练误差减小，泛化误差增大   \n",
        "    seed=42 #每次产生的随机数一样；这里设置随机数种子是因为训练过程中采用了随机采样、随机属性选择   \n",
        ")\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_metric='auc', eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
        "    verbose=True,\n",
        "    #早停法，如果auc在10epoch没有进步就stop\n",
        "    early_stopping_rounds=10 \n",
        ")\n",
        "\n",
        "# model.fit(X_train, y_train)\n",
        "prob = model.predict_proba(test_data) #预测的是标签为0/1的概率分布吗？？？标签-1呢？？？\n",
        "submission['prob'] = pd.Series(prob[:,1])\n",
        "submission.drop(['origin'], axis=1, inplace=True)\n",
        "submission.to_csv('./prediction_xgb.csv', index=False)\n",
        "\n",
        "time2=time.time()\n",
        "print(time2-time1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       user_id  merchant_id label origin    u1  ...  age_7  age_8  g_0  g_1  g_2\n",
            "0       365952         1203   0.0  train    46  ...      0      0    0    1    0\n",
            "1        42624          946   0.0  train   365  ...      0      0    1    0    0\n",
            "2       240000         2278   0.0  train    47  ...      0      0    1    0    0\n",
            "3       177792          951   0.0  train   234  ...      0      0    0    1    0\n",
            "4       322944         1892   0.0  train   186  ...      1      0    1    0    0\n",
            "...        ...          ...   ...    ...   ...  ...    ...    ...  ...  ...  ...\n",
            "23888    47231         1748   nan   test   128  ...      0      0    1    0    0\n",
            "23889    59519          798   nan   test  1286  ...      0      0    1    0    0\n",
            "23890   263039          639   nan   test     9  ...      0      0    0    1    0\n",
            "23891   263039         3954   nan   test     9  ...      0      0    0    1    0\n",
            "23892   423551         2954   nan   test   197  ...      0      0    1    0    0\n",
            "\n",
            "[23893 rows x 47 columns]\n",
            "[0]\tvalidation_0-auc:0.605408\tvalidation_1-auc:0.576259\n",
            "Multiple eval metrics have been passed: 'validation_1-auc' will be used for early stopping.\n",
            "\n",
            "Will train until validation_1-auc hasn't improved in 10 rounds.\n",
            "[1]\tvalidation_0-auc:0.607542\tvalidation_1-auc:0.571348\n",
            "[2]\tvalidation_0-auc:0.607537\tvalidation_1-auc:0.569176\n",
            "[3]\tvalidation_0-auc:0.625346\tvalidation_1-auc:0.586176\n",
            "[4]\tvalidation_0-auc:0.629365\tvalidation_1-auc:0.587037\n",
            "[5]\tvalidation_0-auc:0.632592\tvalidation_1-auc:0.58727\n",
            "[6]\tvalidation_0-auc:0.632022\tvalidation_1-auc:0.587832\n",
            "[7]\tvalidation_0-auc:0.63395\tvalidation_1-auc:0.58571\n",
            "[8]\tvalidation_0-auc:0.633887\tvalidation_1-auc:0.582053\n",
            "[9]\tvalidation_0-auc:0.636091\tvalidation_1-auc:0.584922\n",
            "[10]\tvalidation_0-auc:0.636265\tvalidation_1-auc:0.587109\n",
            "[11]\tvalidation_0-auc:0.638958\tvalidation_1-auc:0.58899\n",
            "[12]\tvalidation_0-auc:0.64234\tvalidation_1-auc:0.592001\n",
            "[13]\tvalidation_0-auc:0.641884\tvalidation_1-auc:0.590646\n",
            "[14]\tvalidation_0-auc:0.640804\tvalidation_1-auc:0.592949\n",
            "[15]\tvalidation_0-auc:0.641338\tvalidation_1-auc:0.595226\n",
            "[16]\tvalidation_0-auc:0.640897\tvalidation_1-auc:0.59445\n",
            "[17]\tvalidation_0-auc:0.639704\tvalidation_1-auc:0.592652\n",
            "[18]\tvalidation_0-auc:0.639963\tvalidation_1-auc:0.595292\n",
            "[19]\tvalidation_0-auc:0.640466\tvalidation_1-auc:0.595856\n",
            "[20]\tvalidation_0-auc:0.640626\tvalidation_1-auc:0.596056\n",
            "[21]\tvalidation_0-auc:0.639582\tvalidation_1-auc:0.594751\n",
            "[22]\tvalidation_0-auc:0.638498\tvalidation_1-auc:0.594151\n",
            "[23]\tvalidation_0-auc:0.639225\tvalidation_1-auc:0.592165\n",
            "[24]\tvalidation_0-auc:0.639217\tvalidation_1-auc:0.593859\n",
            "[25]\tvalidation_0-auc:0.63838\tvalidation_1-auc:0.594306\n",
            "[26]\tvalidation_0-auc:0.638929\tvalidation_1-auc:0.596628\n",
            "[27]\tvalidation_0-auc:0.638368\tvalidation_1-auc:0.595728\n",
            "[28]\tvalidation_0-auc:0.638516\tvalidation_1-auc:0.594954\n",
            "[29]\tvalidation_0-auc:0.638661\tvalidation_1-auc:0.593563\n",
            "[30]\tvalidation_0-auc:0.637764\tvalidation_1-auc:0.594023\n",
            "[31]\tvalidation_0-auc:0.637786\tvalidation_1-auc:0.592549\n",
            "[32]\tvalidation_0-auc:0.637692\tvalidation_1-auc:0.591202\n",
            "[33]\tvalidation_0-auc:0.637146\tvalidation_1-auc:0.590679\n",
            "[34]\tvalidation_0-auc:0.636323\tvalidation_1-auc:0.591033\n",
            "[35]\tvalidation_0-auc:0.636181\tvalidation_1-auc:0.589815\n",
            "[36]\tvalidation_0-auc:0.635789\tvalidation_1-auc:0.589166\n",
            "Stopping. Best iteration:\n",
            "[26]\tvalidation_0-auc:0.638929\tvalidation_1-auc:0.596628\n",
            "\n",
            "30.239185571670532\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}