{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Netflix评分预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Content：对Netflix数据集进行评分预测\n",
    "* Author:  HuiHui\n",
    "* Date:    2020-05-23\n",
    "* Reference:https://www.kaggle.com/netflix-inc/netflix-prize-data/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Netflix数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 该数据集收集了1998年10月至2005年12月之间Netflix用户所有的电影评分数据，涵盖了48万用户对17000部电影的评分记录（超过1亿条），评分范围1-5星\n",
    "* 训练数据：包含了用户对17770部电影的评分  \n",
    "&emsp;电影ID:  \n",
    "&emsp;用户1ID,评分,日期  \n",
    "&emsp;用户2ID,评分,日期  \n",
    "注：电影编号从1到17770;用户ID的范围从1到2649429，有间隙，有480189个用户;评分是从1到5（整数）；日期的格式为yyy-MM-DD。\n",
    "* movie_titles.txt：  \n",
    "&emsp;电影ID,发行年份,片名  \n",
    "* qualifying.txt：  \n",
    "&emsp;电影ID1：  \n",
    "&emsp;客户1ID,日期1  \n",
    "&emsp;客户2ID,日期2  \n",
    "&emsp;电影ID2:  \n",
    "&emsp;客户1ID,日期1  \n",
    "注：您的程序必须根据训练数据集中的信息预测用户在qualifying数据集中为电影提供的所有评分  \n",
    "* probe.txt：  \n",
    "&emsp;电影ID1：  \n",
    "&emsp;客户1ID  \n",
    "&emsp;客户2ID  \n",
    "&emsp;电影ID2:  \n",
    "&emsp;客户1ID   \n",
    "注：与qualifying数据集不同，其每对的评分和日期都包含在培训数据集中；可针对probe探测集计算RMSE与Cinematch的RMSE进行比较\n",
    "* 提交文件：  \n",
    "如果qualifying数据集中，  \n",
    "&emsp;电影ID1：  \n",
    "&emsp;客户1ID,日期1  \n",
    "&emsp;客户2ID,日期2  \n",
    "&emsp;电影ID2:  \n",
    "&emsp;客户1ID,日期1  \n",
    "则预测文件中对应，  \n",
    "&emsp;电影ID1：  \n",
    "&emsp;评分1  \n",
    "&emsp;评分2  \n",
    "&emsp;电影ID2:  \n",
    "&emsp;评分1  \n",
    "注：提交的预测文件的格式应遵循qualifying数据集中电影ID、用户ID、日期的顺序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提交备注"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 说明使用的数据集大小（如果对数据集进行切分）\n",
    "* probe上的RMSE，也可以针对probe子集进行计算\n",
    "* 说明使用的算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入相关库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !/usr/bin/env python\n",
    "# -*- coding=utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time, datetime\n",
    "from surprise import SVD,SVDpp #surprise是推荐算法python实现库\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import Reader\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理：txt-->csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_train_csv(path):\n",
    "    # 以“\\n”为分割符，读取数据，返回df类型\n",
    "    data=pd.read_csv(path,sep=\"\\n\",header=None)\n",
    "\n",
    "    data[\"movieID\"]=data[0].map(lambda x:int(x[:-1]) if  \":\" in x else pd.NaT) # data[0]即第0列\n",
    "    # 用前一个非缺失值去填充缺失的\"movieID\"\n",
    "    data=data.fillna(method=\"ffill\") \n",
    "\n",
    "    # 删除代表movieID的行\n",
    "    mask=data[0].map(lambda x:\":\" not in x) # 结果为True/False\n",
    "    data=data[mask]\n",
    "\n",
    "    data[\"userID\"]=data[0].map(lambda x:int(x.split(\",\")[0]))\n",
    "    data[\"rating\"]=data[0].map(lambda x:int(x.split(\",\")[1]))\n",
    "    data[\"datetime\"]=data[0].map(lambda x:datetime.datetime.strptime(x.split(\",\")[2].strip(), '%Y-%m-%d'))\n",
    "    data= data.drop([0], axis=1)\n",
    "\n",
    "    # 调换movieID userID顺序\n",
    "    data=data.reindex(columns=['userID','movieID','rating','datetime'])\n",
    "    # 行索引重新设置从0开始,并删除原行索引\n",
    "    data=data.reset_index(drop=True)\n",
    "    return data\n",
    "\n",
    "def get_qualifying_csv(path):\n",
    "    data=pd.read_csv(path,sep=\"\\n\",header=None)\n",
    "\n",
    "    data[\"movieID\"]=data[0].map(lambda x:int(x[:-1]) if  \":\" in x else pd.NaT)\n",
    "    data=data.fillna(method=\"ffill\") \n",
    "\n",
    "    mask=data[0].map(lambda x:\":\" not in x)\n",
    "    data=data[mask]\n",
    "\n",
    "    data[\"userID\"]=data[0].map(lambda x:int(x.split(\",\")[0]))\n",
    "    data[\"datetime\"]=data[0].map(lambda x:datetime.datetime.strptime(x.split(\",\")[1].strip(), '%Y-%m-%d'))\n",
    "    data= data.drop([0], axis=1)\n",
    "\n",
    "    data=data.reindex(columns=['userID','movieID','datetime'])\n",
    "    data=data.reset_index(drop=True)\n",
    "    return data\n",
    "\n",
    "def get_probe_csv(path):\n",
    "    data=pd.read_csv(path,sep=\"\\n\",header=None)\n",
    "\n",
    "    data[\"movieID\"]=data[0].map(lambda x:int(x[:-1]) if  \":\" in x else pd.NaT) \n",
    "    data=data.fillna(method=\"ffill\") \n",
    "\n",
    "    mask=data[0].map(lambda x:\":\" not in x) \n",
    "    data=data[mask]\n",
    "\n",
    "    data[\"userID\"]=data[0].map(lambda x:int(x.split(\",\")[0]))\n",
    "    data= data.drop([0], axis=1)\n",
    "\n",
    "    data=data.reindex(columns=['userID','movieID'])\n",
    "    data=data.reset_index(drop=True)\n",
    "    return data\n",
    "\n",
    "# txt-->csv\n",
    "# combined_data_1_csv=get_train_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/combined_data_1.txt\")\n",
    "# combined_data_1_csv.to_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/combined_data_1.csv\",index = False)  # 保存时忽略行索引这一列\n",
    "# combined_data_2_csv=get_train_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/combined_data_2.txt\")\n",
    "# combined_data_2_csv.to_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/combined_data_2.csv\",index = False)\n",
    "# combined_data_3_csv=get_train_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/combined_data_3.txt\")\n",
    "# combined_data_3_csv.to_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/combined_data_3.csv\",index = False)\n",
    "# combined_data_4_csv=get_train_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/combined_data_4.txt\")\n",
    "# combined_data_4_csv.to_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/combined_data_4.csv\",index = False)\n",
    "\n",
    "# probe_csv=get_probe_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/probe.txt\")\n",
    "# probe_csv.to_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/probe.csv\",index = False)\n",
    "\n",
    "# qualifying_csv=get_qualifying_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/qualifying.txt\")\n",
    "# qualifying_csv.to_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/qualifying.csv\",index = False)\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# probe_csv=get_probe_csv(\"probe.txt\")\n",
    "# probe_csv.to_csv(\"probe.csv\",index = False) # 保存时忽略行索引这一列\n",
    "# combined_data_1_csv=get_train_csv(\"combined_data_1.txt\")\n",
    "# combined_data_1_csv.to_csv(\"combined_data_1.csv\",index = False)\n",
    "# combined_data_2_csv=get_train_csv(\"combined_data_2.txt\")\n",
    "# combined_data_2_csv.to_csv(\"combined_data_2.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理：在训练集中查找probe集的评分及日期"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    userID  movieID\n",
      "0    30878        1\n",
      "1  2647871        1\n",
      "2  1283744        1\n",
      "3  2488120        1\n",
      "4   317050        1\n",
      "    userID  movieID  rating    datetime\n",
      "0  1488844        1       3  2005-09-06\n",
      "1   822109        1       5  2005-05-13\n",
      "2   885013        1       4  2005-10-19\n",
      "3    30878        1       4  2005-12-26\n",
      "4   823519        1       3  2004-05-03\n",
      "    userID  movieID  rating    datetime\n",
      "0    30878        1       4  2005-12-26\n",
      "1  2647871        1       4  2005-12-30\n",
      "2  1283744        1       3  2004-04-17\n",
      "3  2488120        1       5  2005-09-20\n",
      "4   317050        1       5  2005-11-15\n"
     ]
    }
   ],
   "source": [
    "## csv需要重新运行\n",
    "\n",
    "# 读取probe.csv\n",
    "probe_csv=pd.read_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/probe.csv\")\n",
    "print(probe_csv.head())\n",
    "\n",
    "# 读取训练数据csv\n",
    "combined_data_1_csv=pd.read_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/combined_data_1.csv\")\n",
    "combined_data_2_csv=pd.read_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/combined_data_2.csv\")\n",
    "combined_data_3_csv=pd.read_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/combined_data_3.csv\")\n",
    "combined_data_4_csv=pd.read_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/combined_data_4.csv\")\n",
    "\n",
    "# 先将所有训练集拼接，再将probe与训练集df合并（左连接），即可得到其对应的评分及日期\n",
    "train_data=pd.concat([combined_data_1_csv,combined_data_2_csv],ignore_index=True) # ignore_index=True表重建索引\n",
    "train_data=pd.concat([train_data,combined_data_3_csv],ignore_index=True)\n",
    "train_data=pd.concat([train_data,combined_data_4_csv],ignore_index=True)\n",
    "print(train_data.head()) \n",
    "train_data.to_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/train_data.csv\",index = False) # 保存成train_data.csv\n",
    "probe_csv=pd.merge(probe_csv,train_data,on=['userID','movieID'],how='left')\n",
    "print(probe_csv.head())\n",
    "\n",
    "# 保存到probe.csv\n",
    "probe_csv.to_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/probe.csv\",index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取train_data.csv\n",
    "train_data=pd.read_csv(\"train_data.csv\")\n",
    "print(train_data.head(20)) \n",
    "\n",
    "# 采集小样本\n",
    " #有什么需要注意的？\n",
    "\n",
    "# 保存小样本train_data_small.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练（使用SVD算法）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据读取\n",
    "reader = Reader(line_format='user item rating timestamp', sep=',', skip_lines=1) # skip_lines=1 读取时跳过第一行即表头\n",
    "\n",
    "# 全量数据\n",
    "data = Dataset.load_from_file('train_data.csv', reader=reader)\n",
    "# 小样本\n",
    "# data = Dataset.load_from_file('train_data_small.csv', reader=reader)\n",
    "\n",
    "train_set = data.build_full_trainset() #不要将数据集拆分或折叠，返回从整个数据集生成的训练集\n",
    "\n",
    "# 使用funkSVD\n",
    "algo = SVD(biased=False)\n",
    "\n",
    "# 这里用train_set训练模型,probe做验证集，最终对qualifying预测，并保存为csv-->txt\n",
    "\n",
    "# 定义K折交叉验证迭代器，K=3\n",
    "kf = KFold(n_splits=3)\n",
    "for trainset, testset in kf.split(data):\n",
    "    # 训练并预测\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    # 计算RMSE\n",
    "    accuracy.rmse(predictions, verbose=True)\n",
    "\n",
    "# uid = str(196)\n",
    "# iid = str(302)\n",
    "# # 输出uid对iid的预测结果\n",
    "# pred = algo.predict(uid, iid, r_ui=4, verbose=True)\n",
    "\n",
    "# sample random trainset and testset\n",
    "# test set is made of 25% of the ratings.\n",
    "trainset, testset = train_test_split(data, test_size=.25)\n",
    "\n",
    "# We'll use the famous SVD algorithm.\n",
    "algo = SVD()\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)\n",
    " #######\n",
    "# Compute biased accuracy on A\n",
    "predictions = algo.test(trainset.build_testset())\n",
    "print('Biased accuracy on A,', end='   ')\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 针对probe计算RMSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提交结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对qualifying进行预测\n",
    "\n",
    "# csv-->txt函数\n",
    "# 保存txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
