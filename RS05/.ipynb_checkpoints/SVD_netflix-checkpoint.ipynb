{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Netflix评分预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Content：对Netflix数据集进行评分预测\n",
    "* Author:  HuiHui\n",
    "* Date:    2020-05-23\n",
    "* Reference:https://www.kaggle.com/netflix-inc/netflix-prize-data/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Netflix数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 该数据集收集了1998年10月至2005年12月之间Netflix用户所有的电影评分数据，涵盖了48万用户对17000部电影的评分记录（超过1亿条），评分范围1-5星\n",
    "* 训练数据：包含了用户对17770部电影的评分  \n",
    "&emsp;电影ID:  \n",
    "&emsp;用户1ID,评分,日期  \n",
    "&emsp;用户2ID,评分,日期  \n",
    "注：电影编号从1到17770;用户ID的范围从1到2649429，有间隙，有480189个用户;评分是从1到5（整数）；日期的格式为yyy-MM-DD。\n",
    "* movie_titles.txt：  \n",
    "&emsp;电影ID,发行年份,片名  \n",
    "* qualifying.txt：  \n",
    "&emsp;电影ID1：  \n",
    "&emsp;客户1ID,日期1  \n",
    "&emsp;客户2ID,日期2  \n",
    "&emsp;电影ID2:  \n",
    "&emsp;客户1ID,日期1  \n",
    "注：您的程序必须根据训练数据集中的信息预测用户在qualifying数据集中为电影提供的所有评分  \n",
    "* probe.txt：  \n",
    "&emsp;电影ID1：  \n",
    "&emsp;客户1ID  \n",
    "&emsp;客户2ID  \n",
    "&emsp;电影ID2:  \n",
    "&emsp;客户1ID   \n",
    "注：与qualifying数据集不同，其每对的评分和日期都包含在培训数据集中；可针对probe探测集计算RMSE与Cinematch的RMSE进行比较\n",
    "* 提交文件：  \n",
    "如果qualifying数据集中，  \n",
    "&emsp;电影ID1：  \n",
    "&emsp;客户1ID,日期1  \n",
    "&emsp;客户2ID,日期2  \n",
    "&emsp;电影ID2:  \n",
    "&emsp;客户1ID,日期1  \n",
    "则预测文件中对应，  \n",
    "&emsp;电影ID1：  \n",
    "&emsp;评分1  \n",
    "&emsp;评分2  \n",
    "&emsp;电影ID2:  \n",
    "&emsp;评分1  \n",
    "注：提交的预测文件的格式应遵循qualifying数据集中电影ID、用户ID、日期的顺序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提交备注"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 说明使用的数据集大小（如果对数据集进行切分）\n",
    "* probe上的RMSE，也可以针对probe子集进行计算\n",
    "* 说明使用的算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入相关库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/usr/bin/env python\n",
    "# -*- coding=utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time, datetime\n",
    "from surprise import SVD,SVDpp #surprise是推荐算法python实现库\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import Reader\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import KFold\n",
    "from surprise.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理：txt-->csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "def get_train_csv(path):\n",
    "    # 以“\\n”为分割符，读取数据，返回df类型\n",
    "    data=pd.read_csv(path,sep=\"\\n\",header=None)\n",
    "\n",
    "    data[\"movieID\"]=data[0].map(lambda x:int(x[:-1]) if  \":\" in x else pd.NaT) # data[0]即第0列\n",
    "    # 用前一个非缺失值去填充缺失的\"movieID\"\n",
    "    data=data.fillna(method=\"ffill\") \n",
    "\n",
    "    # 删除代表movieID的行\n",
    "    mask=data[0].map(lambda x:\":\" not in x) # 结果为True/False\n",
    "    data=data[mask]\n",
    "\n",
    "    data[\"userID\"]=data[0].map(lambda x:int(x.split(\",\")[0]))\n",
    "    data[\"rating\"]=data[0].map(lambda x:int(x.split(\",\")[1]))\n",
    "    data[\"datetime\"]=data[0].map(lambda x:datetime.datetime.strptime(x.split(\",\")[2].strip(), '%Y-%m-%d'))\n",
    "    data= data.drop([0], axis=1)\n",
    "\n",
    "    # 调换movieID userID顺序\n",
    "    data=data.reindex(columns=['userID','movieID','rating','datetime'])\n",
    "    # 行索引重新设置从0开始,并删除原行索引\n",
    "    data=data.reset_index(drop=True)\n",
    "    return data\n",
    "\n",
    "def get_qualifying_csv(path):\n",
    "    data=pd.read_csv(path,sep=\"\\n\",header=None)\n",
    "\n",
    "    data[\"movieID\"]=data[0].map(lambda x:int(x[:-1]) if  \":\" in x else pd.NaT)\n",
    "    data=data.fillna(method=\"ffill\") \n",
    "\n",
    "    mask=data[0].map(lambda x:\":\" not in x)\n",
    "    data=data[mask]\n",
    "\n",
    "    data[\"userID\"]=data[0].map(lambda x:int(x.split(\",\")[0]))\n",
    "    data[\"datetime\"]=data[0].map(lambda x:datetime.datetime.strptime(x.split(\",\")[1].strip(), '%Y-%m-%d'))\n",
    "    data= data.drop([0], axis=1)\n",
    "\n",
    "    data=data.reindex(columns=['userID','movieID','datetime'])\n",
    "    data=data.reset_index(drop=True)\n",
    "    return data\n",
    "\n",
    "def get_probe_csv(path):\n",
    "    data=pd.read_csv(path,sep=\"\\n\",header=None)\n",
    "\n",
    "    data[\"movieID\"]=data[0].map(lambda x:int(x[:-1]) if  \":\" in x else pd.NaT) \n",
    "    data=data.fillna(method=\"ffill\") \n",
    "\n",
    "    mask=data[0].map(lambda x:\":\" not in x) \n",
    "    data=data[mask]\n",
    "\n",
    "    data[\"userID\"]=data[0].map(lambda x:int(x.split(\",\")[0]))\n",
    "    data= data.drop([0], axis=1)\n",
    "\n",
    "    data=data.reindex(columns=['userID','movieID'])\n",
    "    data=data.reset_index(drop=True)\n",
    "    return data\n",
    "\n",
    "# txt-->csv\n",
    "# combined_data_1_csv=get_train_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/combined_data_1.txt\")\n",
    "# combined_data_1_csv.to_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/combined_data_1.csv\",index = False)  # 保存时忽略行索引这一列\n",
    "# combined_data_2_csv=get_train_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/combined_data_2.txt\")\n",
    "# combined_data_2_csv.to_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/combined_data_2.csv\",index = False)\n",
    "# combined_data_3_csv=get_train_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/combined_data_3.txt\")\n",
    "# combined_data_3_csv.to_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/combined_data_3.csv\",index = False)\n",
    "# combined_data_4_csv=get_train_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/combined_data_4.txt\")\n",
    "# combined_data_4_csv.to_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/combined_data_4.csv\",index = False)\n",
    "\n",
    "# probe_csv=get_probe_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/probe.txt\")\n",
    "# probe_csv.to_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/probe.csv\",index = False)\n",
    "\n",
    "# qualifying_csv=get_qualifying_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/qualifying.txt\")\n",
    "# qualifying_csv.to_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/qualifying.csv\",index = False)\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    userID  movieID  rating    datetime\n",
      "0  1488844        1     3.0  2020-01-01\n",
      "1    30878        1     0.0  2020-01-01\n",
      "2  1227322        2     4.0  2020-01-01\n",
      "3  1009622        3     2.0  2020-01-01\n"
     ]
    }
   ],
   "source": [
    "# combined_data_1_csv=get_train_csv(\"combined_data_1.txt\")\n",
    "# combined_data_1_csv.to_csv(\"combined_data_1.csv\",index = False)\n",
    "# combined_data_2_csv=get_train_csv(\"combined_data_2.txt\")\n",
    "# combined_data_2_csv.to_csv(\"combined_data_2.csv\",index = False)\n",
    "\n",
    "# probe_csv=get_probe_csv(\"probe.txt\")\n",
    "# probe_csv.to_csv(\"probe.csv\",index = False) # 保存时忽略行索引这一列\n",
    "\n",
    "# qualifying_csv=get_qualifying_csv(\"qualifying.txt\")\n",
    "# qualifying_csv.to_csv(\"qualifying.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理：在训练集中查找probe集的评分及日期"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取probe.csv\n",
    "probe_csv=pd.read_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/probe.csv\")\n",
    "print(probe_csv.head())\n",
    "\n",
    "# 读取训练数据csv\n",
    "combined_data_1_csv=pd.read_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/combined_data_1.csv\")\n",
    "combined_data_2_csv=pd.read_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/combined_data_2.csv\")\n",
    "combined_data_3_csv=pd.read_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/combined_data_3.csv\")\n",
    "combined_data_4_csv=pd.read_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/combined_data_4.csv\")\n",
    "\n",
    "# 先将所有训练集拼接，再将probe与训练集df合并（左连接），即可得到其对应的评分及日期\n",
    "train_data=pd.concat([combined_data_1_csv,combined_data_2_csv],ignore_index=True) # ignore_index=True表重建索引\n",
    "train_data=pd.concat([train_data,combined_data_3_csv],ignore_index=True)\n",
    "train_data=pd.concat([train_data,combined_data_4_csv],ignore_index=True)\n",
    "print(train_data.head()) \n",
    "train_data.to_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/train_data.csv\",index = False) # 保存成train_data.csv\n",
    "probe_csv=pd.merge(probe_csv,train_data,on=['userID','movieID'],how='left')\n",
    "print(probe_csv.head())\n",
    "\n",
    "# 保存到probe.csv\n",
    "probe_csv.to_csv(\"/Users/wangdonghui/Desktop/ZGZ/RS/dataset/netflix-prize-data/probe.csv\",index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    userID  movieID  rating    datetime\n",
      "0  1481961        3       0  2020-01-01\n",
      "1    30878        1       0  2020-01-01\n",
      "2  8221093        1       4  2020-01-01\n",
      "3  1009622        3       2  2020-01-01\n",
      "4  1488844        1       3  2020-01-01\n",
      "5  1227322        2       4  2020-01-01\n"
     ]
    }
   ],
   "source": [
    "# #￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥￥更改、运行\n",
    "\n",
    "# 读取train_data.csv\n",
    "train_data=pd.read_csv(\"train_data.csv\")\n",
    "\n",
    "# 训练数据集随机采样\n",
    "train_data_small = train_data.sample(frac=0.01, replace=False, random_state=1) # frac=0.8抽取比例；replace=False未放回抽样；使用random_state，以确保可重复性实现\n",
    "train_data_small = train_data_small.reset_index(drop=True)\n",
    "print(train_data_small.head(10)) \n",
    "\n",
    "# 保存小样本train_data_small.csv\n",
    "train_data_small.to_csv(\"train_data_small.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评分预测（使用SVD算法）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "[Prediction(uid='1488844', iid='1', r_ui=3.0, est=1, details={'was_impossible': False}), Prediction(uid='30878', iid='1', r_ui=0.0, est=1, details={'was_impossible': False}), Prediction(uid='1227322', iid='2', r_ui=4.0, est=1, details={'was_impossible': False}), Prediction(uid='1009622', iid='3', r_ui=2.0, est=1, details={'was_impossible': False})]\n",
      "RMSE: 1.9365\n",
      "[('1181550', '2', 0), ('1227322', '2', 0), ('885014', '2', 0), ('1009622', '3', 0), ('1481963', '3', 0)]\n",
      "    userID  movieID    rating    datetime\n",
      "0  1181550        2  2.166667  2020-01-01\n",
      "1  1227322        2  1.000000  2020-01-01\n",
      "2   885014        2  2.166667  2020-01-01\n",
      "3  1009622        3  1.000000  2020-01-01\n",
      "4  1481963        3  2.166667  2020-01-01\n"
     ]
    }
   ],
   "source": [
    "# 数据读取\n",
    "reader = Reader(line_format='user item rating timestamp', sep=',', skip_lines=1) # skip_lines=1 读取时跳过第一行即表头\n",
    "\n",
    "# # 全量数据\n",
    "# train_data = Dataset.load_from_file('train_data.csv', reader=reader)\n",
    "# 小样本\n",
    "train_data = Dataset.load_from_file('train_data_small.csv', reader=reader)\n",
    "probe_data = Dataset.load_from_file('probe.csv', reader=reader)\n",
    "\n",
    "trainset = train_data.build_full_trainset() # 不将数据集拆分或折叠，返回从整个数据集生成的训练集\n",
    "probeset = probe_data.build_full_trainset().build_testset()\n",
    "print(type(probeset[0]))\n",
    "\n",
    "# 使用funkSVD\n",
    "algo = SVD(biased=False)\n",
    "\n",
    "# 训练模型\n",
    "algo.fit(trainset)\n",
    "\n",
    "# 计算RMSE（probe）\n",
    "predictions = algo.test(probeset)\n",
    "print(predictions)\n",
    "accuracy.rmse(predictions,verbose=True) # verbose=True打印预测详细信息\n",
    "\n",
    "# 预测（qualifying）\n",
    "results = pd.read_csv(\"qualifying.csv\")\n",
    "testset = [(str(row[1]),str(row[2]),0) for row in results.itertuples()]\n",
    "print(testset)\n",
    "results[\"rating\"] = algo.test(testset)\n",
    "results[\"rating\"]=results[\"rating\"].map(lambda x:x[3])\n",
    "results = results.reindex(columns=['userID','movieID','rating','datetime'])\n",
    "print(results.head(10))\n",
    "\n",
    "# 结果保存为csv\n",
    "results.to_csv(\"results.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 若预测集中出现训练集中没有的新用户或新电影，矩阵分解(SVD)是如何给出预测评分的呢？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
